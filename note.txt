# this for refrence

from selenium import webdriver  # allows launch web browser
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.chrome.service import Service

import pandas as pd


driver_options = webdriver.ChromeOptions()
driver_options.add_argument("-incognito")
driver_options.add_argument("--ignore-certificate-error")
chromedriver_path = (
    r"C:\Users\sabar\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
)


def create_webdriver():
    service = Service(chromedriver_path)
    return webdriver.Chrome(service=service, options=driver_options)


browser = create_webdriver()
browser.get("https://github.com/collections/machine-learning")

projects = browser.find_elements(By.XPATH, "//h1[@class='h3 lh-condensed']") # for scraping just name and url



# extracting info of projects
# project_list = {}

for i in projects:
    project_name = i.text
    project_url = i.find_elements(By.TAG_NAME, "a")[0].get_attribute("href")
    project_list[project_name] = project_url
    


browser.quit()
project_df = pd.DataFrame.from_dict(project_list, orient="index")
project_df["project_name"] = project_df.index
project_df.columns = ["project_url", "project_name"]
project_df = project_df.reset_index(drop=True)


project_df.to_csv("project_list.csv")

# the above code will scrape url and name 



# this one scrape description, tech, and star_count 

from selenium import webdriver  # allows launch web browser
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.chrome.service import Service

import pandas as pd


driver_options = webdriver.ChromeOptions()
driver_options.add_argument("-incognito")
driver_options.add_argument("--ignore-certificate-error")
chromedriver_path = (
    r"C:\Users\sabar\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
)


def create_webdriver():
    service = Service(chromedriver_path)
    return webdriver.Chrome(service=service, options=driver_options)


browser = create_webdriver()
browser.get("https://github.com/collections/machine-learning")

# projects = browser.find_elements(By.XPATH, "//h1[@class='h3 lh-condensed']") # for scraping just name and url
projects = browser.find_elements(
    By.XPATH,
    "//article[@class='height-full border color-border-muted rounded-2 p-3 p-md-5 my-5']",
)


# extracting info of projects
# project_list = {}
project_list = []
for i in projects:
    project_name = i.text
    project_url = i.find_elements(By.TAG_NAME, "a")[0].get_attribute("href")
    desc_ele = i.find_elements(
        By.XPATH, ".//div[@class='color-fg-muted mb-2 ws-normal']"
    )
    description = desc_ele[0].text if desc_ele else "N/A"
    star_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'stargazers')]")
    star_count = star_ele[0].text if star_ele else "0"
    fork_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'forks')]")
    forks = fork_ele[0].text if fork_ele else "0"
    tech_ele = i.find_elements(By.XPATH, ".//span[@itemprop='programmingLanguage']")
    technology_used = tech_ele[0].text if tech_ele else "N/A"
    # project_list[project_name] = project_url
    project_list.append(
        {
            "Project Name": project_name,
            "Project URL": project_url,
            "Description": description,
            "Stars": star_count,
            "Forks": forks,
            "Technology Used": technology_used,
        }
    )


browser.quit()
# project_df = pd.DataFrame.from_dict(project_list, orient="index")
# project_df["project_name"] = project_df.index
# project_df.columns = ["project_url", "project_name","description","star_count","forks","techonolgy_used"]
# project_df = project_df.reset_index(drop=True)
project_df = pd.DataFrame(project_list)

project_df.to_csv("project_list.csv")


# this one will scrape everthing till 
from selenium import webdriver  # allows launch web browser
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains

import pandas as pd
import time


driver_options = webdriver.ChromeOptions()
driver_options.add_argument("-incognito")
driver_options.add_argument("--ignore-certificate-error")
driver_options.add_argument("--ignore-ssl-errors")
driver_options.add_argument("--disable-gpu")
driver_options.add_argument("--headless")# it won't open pop chrome 
chromedriver_path = (
    r"C:\Users\sabar\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
)


def create_webdriver():
    service = Service(chromedriver_path)
    return webdriver.Chrome(service=service, options=driver_options)


browser = create_webdriver()
browser.get("https://github.com/collections/machine-learning")

# for scrolling ifinite or if it has load button
wait = WebDriverWait(browser, 10)
action = ActionChains(browser)
scroll_pause_time = 1
screen_height = browser.execute_script("return window.screen.height;")
i = 1

while True:
    browser.execute_script("window.scrollTo(0, {});".format(i * screen_height))
    i += 1
    time.sleep(scroll_pause_time)

    try:
        load_more_button = wait.until(
            expected_conditions.element_to_be_clickable(
                (By.XPATH, "//button[contains(text(),'Load more')]")
            )
        )
        action.move_to_element(load_more_button).click().perform()
        time.sleep(2)

    except:
        pass

    scroll_height = browser.execute_script("return document.body.scrollHeight;")
    if (screen_height * i) > scroll_height:
        print("Reached bottom of the page.")
        break

# projects = browser.find_elements(By.XPATH, "//h1[@class='h3 lh-condensed']") # for scraping just name and url
projects = browser.find_elements(
    By.XPATH,
    "//article[@class='height-full border color-border-muted rounded-2 p-3 p-md-5 my-5']",
)


# extracting info of projects
# project_list = {}
project_list = []
for i in projects:
    project_name = i.text
    project_url = i.find_elements(By.TAG_NAME, "a")[0].get_attribute("href")
    desc_ele = i.find_elements(
        By.XPATH, ".//div[@class='color-fg-muted mb-2 ws-normal']"
    )
    description = desc_ele[0].text if desc_ele else "N/A"
    star_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'stargazers')]")
    star_count = star_ele[0].text if star_ele else "0"
    fork_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'forks')]")
    forks = fork_ele[0].text if fork_ele else "0"
    tech_ele = i.find_elements(By.XPATH, ".//span[@itemprop='programmingLanguage']")
    technology_used = tech_ele[0].text if tech_ele else "N/A"
    # project_list[project_name] = project_url
    project_list.append(
        {
            "Project Name": project_name,
            "Project URL": project_url,
            "Description": description,
            "Stars": star_count,
            "Forks": forks,
            "Technology Used": technology_used,
        }
    )


browser.quit()
# project_df = pd.DataFrame.from_dict(project_list, orient="index")
# project_df["project_name"] = project_df.index
# project_df.columns = ["project_url", "project_name","description","star_count","forks","techonolgy_used"]
# project_df = project_df.reset_index(drop=True)
project_df = pd.DataFrame(project_list)

project_df.to_csv("project_list.csv")


# this is advance with streamlit function 

import streamlit as st

st.title("scraped info")


from selenium import webdriver  # allows launch web browser
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains

import pandas as pd
import time


def create_webdriver():
    driver_options = webdriver.ChromeOptions()
    driver_options.add_argument("-incognito")
    driver_options.add_argument("--ignore-certificate-error")
    driver_options.add_argument("--ignore-ssl-errors")
    driver_options.add_argument("--disable-gpu")
    driver_options.add_argument("--headless")  # it won't open pop chrome
    chromedriver_path = r"C:\Users\sabar\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
    service = Service(chromedriver_path)
    return webdriver.Chrome(service=service, options=driver_options)


def scrap_git_repo():

    browser = create_webdriver()
    browser.get("https://github.com/collections/machine-learning")

    # for scrolling ifinite or if it has load button
    wait = WebDriverWait(browser, 10)
    action = ActionChains(browser)
    scroll_pause_time = 1
    screen_height = browser.execute_script("return window.screen.height;")
    i = 1

    while True:
        browser.execute_script("window.scrollTo(0, {});".format(i * screen_height))
        i += 1
        time.sleep(scroll_pause_time)

        try:
            load_more_button = wait.until(
                expected_conditions.element_to_be_clickable(
                    (By.XPATH, "//button[contains(text(),'Load more')]")
                )
            )
            action.move_to_element(load_more_button).click().perform()
            time.sleep(2)

        except:
            pass

        scroll_height = browser.execute_script("return document.body.scrollHeight;")
        if (screen_height * i) > scroll_height:
            print("Reached bottom of the page.")
            break

        # projects = browser.find_elements(By.XPATH, "//h1[@class='h3 lh-condensed']") # for scraping just name and url
    projects = browser.find_elements(
        By.XPATH,
        "//article[@class='height-full border color-border-muted rounded-2 p-3 p-md-5 my-5']",
    )

    # extracting info of projects
    # project_list = {}
    project_list = []
    for i in projects:
        project_name = i.text
        project_url = i.find_elements(By.TAG_NAME, "a")[0].get_attribute("href")
        desc_ele = i.find_elements(
            By.XPATH, ".//div[@class='color-fg-muted mb-2 ws-normal']"
        )
        description = desc_ele[0].text if desc_ele else "N/A"
        star_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'stargazers')]")
        star_count = star_ele[0].text if star_ele else "0"
        fork_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'forks')]")
        forks = fork_ele[0].text if fork_ele else "0"
        tech_ele = i.find_elements(By.XPATH, ".//span[@itemprop='programmingLanguage']")
        technology_used = tech_ele[0].text if tech_ele else "N/A"
        # project_list[project_name] = project_url
        project_list.append(
            {
                "Project Name": project_name,
                "Project URL": project_url,
                "Description": description,
                "Stars": star_count,
                "Forks": forks,
                "Technology Used": technology_used,
            }
        )

    browser.quit()
    return pd.DataFrame(project_list)

    # project_df = pd.DataFrame.from_dict(project_list, orient="index")
    # project_df["project_name"] = project_df.index
    # project_df.columns = ["project_url", "project_name","description","star_count","forks","techonolgy_used"]
    # project_df = project_df.reset_index(drop=True)
    # project_df = pd.DataFrame(project_list)

    # project_df.to_csv("project_list.csv")


st.title("GitHub ML Collection Scraper")
st.write("Click to scrape.")

if st.button("Scrape"):
    with st.spinner("Scraping... Please wait!"):
        df = scrap_git_repo()
        st.success("Scraping completed!")
        st.dataframe(df)

        csv = df.to_csv(index=False)
        st.download_button(
            "Download CSV ", data=csv, file_name="project_lists.csv", mime="text/csv"
        )

# need to modify and look into big pitures


# this one is working but still need modification it has filter and Visualization

import streamlit as st


from selenium import webdriver  # allows launch web browser
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
import plotly.express as px
import pandas as pd
import time

st.set_page_config(page_title="Github scraper", layout="wide")
st.title("GitHub Machine Learning Scraper")
st.sidebar.header("Customization Scraper")
github_url = st.sidebar.text_input(
    "Enter the GitHub repository URL", "https://github.com/collections/machine-learning"
)


def create_webdriver():
    driver_options = webdriver.ChromeOptions()
    driver_options.add_argument("-incognito")
    driver_options.add_argument("--ignore-certificate-error")
    driver_options.add_argument("--ignore-ssl-errors")
    driver_options.add_argument("--disable-gpu")
    driver_options.add_argument("--headless")  # it won't open pop chrome
    chromedriver_path = r"C:\Users\sabar\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
    service = Service(chromedriver_path)
    return webdriver.Chrome(service=service, options=driver_options)


def scrap_git_repo(url):

    browser = create_webdriver()
    browser.get(url)

    # for scrolling ifinite or if it has load button
    wait = WebDriverWait(browser, 10)
    action = ActionChains(browser)
    scroll_pause_time = 1
    screen_height = browser.execute_script("return window.screen.height;")
    i = 1

    while True:
        browser.execute_script("window.scrollTo(0, {});".format(i * screen_height))
        i += 1
        time.sleep(scroll_pause_time)

        try:
            load_more_button = wait.until(
                expected_conditions.element_to_be_clickable(
                    (By.XPATH, "//button[contains(text(),'Load more')]")
                )
            )
            action.move_to_element(load_more_button).click().perform()
            time.sleep(2)

        except:
            pass

        scroll_height = browser.execute_script("return document.body.scrollHeight;")
        if (screen_height * i) > scroll_height:
            print("Reached bottom of the page.")
            break

        # projects = browser.find_elements(By.XPATH, "//h1[@class='h3 lh-condensed']") # for scraping just name and url
    projects = browser.find_elements(
        By.XPATH,
        "//article[@class='height-full border color-border-muted rounded-2 p-3 p-md-5 my-5']",
    )

    # extracting info of projects
    # project_list = {}
    project_list = []
    for i in projects:
        project_name = i.text
        project_url = i.find_elements(By.TAG_NAME, "a")[0].get_attribute("href")
        desc_ele = i.find_elements(
            By.XPATH, ".//div[@class='color-fg-muted mb-2 ws-normal']"
        )
        description = desc_ele[0].text if desc_ele else "N/A"
        star_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'stargazers')]")
        star_count = star_ele[0].text if star_ele else "0"
        fork_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'forks')]")
        forks = fork_ele[0].text if fork_ele else "0"
        tech_ele = i.find_elements(By.XPATH, ".//span[@itemprop='programmingLanguage']")
        technology_used = tech_ele[0].text if tech_ele else "N/A"
        # project_list[project_name] = project_url
        project_list.append(
            {
                "Project Name": project_name,
                "Project URL": project_url,
                "Description": description,
                "Stars": star_count,
                "Forks": forks,
                "Technology Used": technology_used,
            }
        )

    browser.quit()
    return pd.DataFrame(project_list)

    # project_df = pd.DataFrame.from_dict(project_list, orient="index")
    # project_df["project_name"] = project_df.index
    # project_df.columns = ["project_url", "project_name","description","star_count","forks","techonolgy_used"]
    # project_df = project_df.reset_index(drop=True)
    # project_df = pd.DataFrame(project_list)

    # project_df.to_csv("project_list.csv")


if st.sidebar.button("Start Scraping"):
    with st.spinner("Scraping GitHub Collection..."):
        df = scrap_git_repo(github_url)
        st.success("Scraping Completed!")

        # Show Data
        st.write("### Scraped Repositories")
        st.dataframe(df)

        # CSV Download Button
        csv = df.to_csv(index=False)
        st.download_button(
            " Download CSV", data=csv, file_name="github_projects.csv", mime="text/csv"
        )

        # Filters

        if not df.empty:
            df["Stars"] = df["Stars"].astype(int)
            max_stars = int(df["Stars"].max())
            min_stars = st.sidebar.slider(
                "Minimum Stars:", min_value=0, max_value=max_stars, value=0
            )

            df_filtered = df[df["Stars"] >= min_stars]

        # Top Repositories Visualization
        st.write("### Top Starred Repositories")
        # fig = px.bar(
        #     df_filtered.sort_values(by="Stars", ascending=False)[:10],
        #     x="Stars",
        #     y="Project Name",
        #     orientation="h",
        #     title="Top Starred Repositories",
        # )
        # st.plotly_chart(fig)
     
        # Most Used Languages
        st.write("### Most Used Technologies")
        lang_counts = df["Technology Used"].value_counts().reset_index()
        lang_counts.columns = ["Technology", "Count"]
        fig2 = px.pie(
            lang_counts,
            names="Technology",
            values="Count",
            title="Most Popular Technologies",
        )
        st.plotly_chart(fig2)




# well fine code 

import streamlit as st
import plotly.express as px
import pandas as pd
import time
import random


from concurrent.futures import ThreadPoolExecutor, as_completed
import concurrent.futures


from selenium import webdriver  # allows launch web browser
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium_stealth import stealth


st.set_page_config(page_title="Github scraper", layout="wide")
st.title("GitHub Machine Learning Scraper")
st.sidebar.header("Customization Scraper")
github_url = st.sidebar.text_input(
    "Enter the GitHub repository URL", "https://github.com/collections/machine-learning"
)
if "scraped_data" not in st.session_state:
    st.session_state.scraped_data = None


def create_webdriver():
    driver_options = webdriver.ChromeOptions()
    driver_options.add_argument("-incognito")
    driver_options.add_argument("--ignore-certificate-error")
    driver_options.add_argument("--ignore-ssl-errors")
    driver_options.add_argument("--disable-gpu")
    driver_options.add_argument("--headless")  # it won't open pop chrome
    driver_options.add_argument("--disable-popup-blocking")
    driver_options.add_argument("--disable-blink-features=AutomationControlled")
    driver_options.add_argument("--disable-extensions")
    driver_options.add_argument("--disable-dev-shm-usage")
    driver_options.add_argument("--start-maximized")
    driver_options.add_argument("--no-sandbox")

    # chromedriver_path = r"C:\Users\sabar\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
    # service = Service(chromedriver_path)
    # return webdriver.Chrome(service=service, options=driver_options)

    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15",
    ]
    user_agent = random.choice(user_agents)
    driver_options.add_argument(f"user-agent={user_agent}")

    chromedriver_path = r"C:\Users\sabar\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
    service = Service(chromedriver_path)
    driver = webdriver.Chrome(service=service, options=driver_options)

    # Apply stealth settings
    stealth(
        driver,
        languages=["en-US", "en"],
        vendor="Google Inc.",
        platform="Win32",
        webgl_vendor="Intel Inc.",
        renderer="Intel Iris OpenGL Engine",
        fix_hairline=True,
    )

    return driver


def scrap_git_repo(url):

    browser = create_webdriver()
    browser.get(url)

    # for scrolling ifinite or if it has load button
    wait = WebDriverWait(browser, 10)
    action = ActionChains(browser)
    scroll_pause_time = 1
    screen_height = browser.execute_script("return window.screen.height;")
    i = 1

    while True:
        browser.execute_script("window.scrollTo(0, {});".format(i * screen_height))
        i += 1
        time.sleep(scroll_pause_time)

        try:
            load_more_button = wait.until(
                expected_conditions.element_to_be_clickable(
                    (By.XPATH, "//button[contains(text(),'Load more')]")
                )
            )
            action.move_to_element(load_more_button).click().perform()
            time.sleep(2)

        except:
            pass

        scroll_height = browser.execute_script("return document.body.scrollHeight;")
        if (screen_height * i) > scroll_height:
            print("Reached bottom of the page.")
            break

        # projects = browser.find_elements(By.XPATH, "//h1[@class='h3 lh-condensed']") # for scraping just name and url
    projects = browser.find_elements(
        By.XPATH,
        "//article[@class='height-full border color-border-muted rounded-2 p-3 p-md-5 my-5']",
    )

    # extracting info of projects
    # project_list = {}
    project_list = []
    for i in projects:
        project_name = i.text
        project_url = i.find_elements(By.TAG_NAME, "a")[0].get_attribute("href")
        desc_ele = i.find_elements(
            By.XPATH, ".//div[@class='color-fg-muted mb-2 ws-normal']"
        )
        description = desc_ele[0].text if desc_ele else "N/A"
        star_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'stargazers')]")
        star_count = star_ele[0].text if star_ele else "0"
        fork_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'forks')]")
        forks = fork_ele[0].text if fork_ele else "0"
        tech_ele = i.find_elements(By.XPATH, ".//span[@itemprop='programmingLanguage']")
        technology_used = tech_ele[0].text if tech_ele else "N/A"
        # project_list[project_name] = project_url
        project_list.append(
            {
                "Project Name": project_name,
                "Project URL": project_url,
                "Description": description,
                "Stars": star_count,
                "Forks": forks,
                "Technology Used": technology_used,
            }
        )

    browser.quit()
    return pd.DataFrame(project_list)

    # project_df = pd.DataFrame.from_dict(project_list, orient="index")
    # project_df["project_name"] = project_df.index
    # project_df.columns = ["project_url", "project_name","description","star_count","forks","techonolgy_used"]
    # project_df = project_df.reset_index(drop=True)
    # project_df = pd.DataFrame(project_list)

    # project_df.to_csv("project_list.csv")


# if "scraped_data" not in st.session_state:
#     st.session_state.scraped_data = None

if st.sidebar.button("Start Scraping"):
    with st.spinner("Scraping GitHub Collection..."):
        df = scrap_git_repo(github_url)
        st.success("Scraping Completed!")

        # Show Data
        st.write("### Scraped Repositories")
        st.dataframe(df)

        # CSV Download Button
        csv = df.to_csv(index=False)
        st.download_button(
            " Download CSV", data=csv, file_name="github_projects.csv", mime="text/csv"
        )

        # Filters

        if not df.empty:
            df["Stars"] = df["Stars"].astype(int)
            max_stars = int(df["Stars"].max())
            min_stars = st.sidebar.slider(
                "Minimum Stars:", min_value=0, max_value=max_stars, value=0
            )

            df_filtered = df[df["Stars"] >= min_stars]

        # Top Repositories Visualization
        st.write("### Top Starred Repositories")
        fig = px.bar(
            df_filtered.sort_values(by="Stars", ascending=False)[:10],
            x="Stars",
            y="Project Name",
            orientation="h",
            title="Top Starred Repositories",
        )
        st.plotly_chart(fig)

        # if not df.empty:
        #     df["Stars"] = df["Stars"].astype(int)
        #     max_stars = int(df["Stars"].max())
        #     min_stars = st.sidebar.slider(
        #         "Minimum Stars:", min_value=0, max_value=max_stars, value=0
        #     )

        #     df_filtered = df[df["Stars"] >= min_stars]

        #     # **Top Repositories Visualization**
        #     st.write("### Top Starred Repositories")
        #     df_filtered_sorted = df_filtered.sort_values(by="Stars", ascending=False)[
        #         :10
        #     ]

        #     fig = px.bar(
        #         df_filtered_sorted,
        #         x="Stars",
        #         y="Project Name",
        #         orientation="h",
        #         title="⭐ Top Starred GitHub Repositories",
        #         labels={"Stars": "GitHub Stars", "Project Name": "Repository Name"},
        #     )

        #     fig.update_layout(
        #         yaxis=dict(title="Repository Name", tickmode="linear"),
        #         xaxis=dict(title="GitHub Stars", tickformat=","),
        #         margin=dict(l=200, r=50, t=50, b=50),
        #         height=600,
        #     )

        #     st.plotly_chart(fig)

        #     df_filtered = df[df["Stars"] >= min_stars]

        # # Top Repositories Visualization
        # st.write("### Top Starred Repositories")
        # df_filtered_sorted = df_filtered.sort_values(by="Stars", ascending=False)[:10]

        # # Create the bar chart
        # fig = px.bar(
        #     df_filtered_sorted,
        #     x="Stars",
        #     y="Project Name",
        #     orientation="h",
        #     title="⭐ Top Starred GitHub Repositories",
        #     labels={"Stars": "GitHub Stars", "Project Name": "Repository Name"},
        # )

        # fig.update_layout(
        #     yaxis=dict(title="Repository Name", tickmode="linear"),
        #     xaxis=dict(title="GitHub Stars", tickformat=","),
        #     margin=dict(l=200, r=50, t=50, b=50),  # Adjust margins for better spacing
        #     height=600,  # Increase height for better readability
        # )

        # st.plotly_chart(fig)

        # fig = px.bar(
        #     df_filtered.sort_values(by="Stars", ascending=False)[:10],
        #     x="Stars",
        #     y="Project Name",
        #     orientation="h",
        #     title="Top Starred Repositories",
        # )
        # st.plotly_chart(fig)

        # Most Used Languages
        st.write("### Most Used Technologies")
        lang_counts = df["Technology Used"].value_counts().reset_index()
        lang_counts.columns = ["Technology", "Count"]
        fig2 = px.pie(
            lang_counts,
            names="Technology",
            values="Count",
            title="Most Popular Technologies",
        )
        st.plotly_chart(fig2)


# st.write("Click to scrape.")

# if st.button("Scrape"):
#     with st.spinner("Scraping... Please wait!"):
#         df = scrap_git_repo()
#         st.success("Scraping completed!")
#         st.dataframe(df)

#         csv = df.to_csv(index=False)
#         st.download_button(
#             "Download CSV ", data=csv, file_name="project_lists.csv", mime="text/csv"
#         )


# fully functional prototype:

import streamlit as st
import plotly.express as px
import pandas as pd
import time
import random


from concurrent.futures import ThreadPoolExecutor, as_completed
import concurrent.futures


from selenium import webdriver  # allows launch web browser
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium_stealth import stealth


st.set_page_config(page_title="Github scraper", layout="wide")
st.title("GitHub Machine Learning Scraper")
st.sidebar.header("Customization Scraper")
github_url = st.sidebar.text_input(
    "Enter the GitHub repository URL", "https://github.com/collections/machine-learning"
)
if "scraped_data" not in st.session_state:
    st.session_state.scraped_data = None


def create_webdriver():
    driver_options = webdriver.ChromeOptions()
    driver_options.add_argument("-incognito")
    driver_options.add_argument("--ignore-certificate-error")
    driver_options.add_argument("--ignore-ssl-errors")
    driver_options.add_argument("--disable-gpu")
    driver_options.add_argument("--headless")  # it won't open pop chrome
    driver_options.add_argument("--disable-popup-blocking")
    driver_options.add_argument("--disable-blink-features=AutomationControlled")
    driver_options.add_argument("--disable-extensions")
    driver_options.add_argument("--disable-dev-shm-usage")
    driver_options.add_argument("--start-maximized")
    driver_options.add_argument("--no-sandbox")

    # chromedriver_path = r"C:\Users\sabar\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
    # service = Service(chromedriver_path)
    # return webdriver.Chrome(service=service, options=driver_options)

    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15",
    ]
    user_agent = random.choice(user_agents)
    driver_options.add_argument(f"user-agent={user_agent}")

    chromedriver_path = r"C:\Users\sabar\Downloads\chromedriver-win64\chromedriver-win64\chromedriver.exe"
    service = Service(chromedriver_path)
    driver = webdriver.Chrome(service=service, options=driver_options)

    # Apply stealth settings
    stealth(
        driver,
        languages=["en-US", "en"],
        vendor="Google Inc.",
        platform="Win32",
        webgl_vendor="Intel Inc.",
        renderer="Intel Iris OpenGL Engine",
        fix_hairline=True,
    )

    return driver


def scrap_git_repo(url):

    browser = create_webdriver()
    browser.get(url)

    if "Page not found" in browser.title:
        st.error(
            "Error: The GitHub page does not exist (404 Not Found). Please check the URL."
        )
        browser.quit()
        return pd.DataFrame()

    # Check if the page loaded successfully
    if "GitHub" not in browser.title:
        st.error("Error: The provided URL is not a valid GitHub page.")
        browser.quit()
        return pd.DataFrame()

    # for scrolling ifinite or if it has load button
    wait = WebDriverWait(browser, 10)
    action = ActionChains(browser)
    scroll_pause_time = 1
    screen_height = browser.execute_script("return window.screen.height;")
    i = 1

    while True:
        browser.execute_script("window.scrollTo(0, {});".format(i * screen_height))
        i += 1
        time.sleep(scroll_pause_time)

        try:
            load_more_button = wait.until(
                expected_conditions.element_to_be_clickable(
                    (By.XPATH, "//button[contains(text(),'Load more')]")
                )
            )
            action.move_to_element(load_more_button).click().perform()
            time.sleep(2)

        # except:
        #     pass

        except Exception as e:
            print(f"Load more button not found or not clickable: {e}")

        scroll_height = browser.execute_script("return document.body.scrollHeight;")
        if (screen_height * i) > scroll_height:
            print("Reached bottom of the page.")
            break

        # projects = browser.find_elements(By.XPATH, "//h1[@class='h3 lh-condensed']") # for scraping just name and url
    projects = browser.find_elements(
        By.XPATH,
        "//article[@class='height-full border color-border-muted rounded-2 p-3 p-md-5 my-5']",
    )
    if not projects:
        st.warning("No repositories found. The page structure might have changed.")
        return []

    # extracting info of projects
    # project_list = {}
    project_list = []
    for i in projects:
        try:
            project_name = i.text
            project_url = i.find_elements(By.TAG_NAME, "a")[0].get_attribute("href")
            desc_ele = i.find_elements(
                By.XPATH, ".//div[@class='color-fg-muted mb-2 ws-normal']"
            )
            description = desc_ele[0].text if desc_ele else "N/A"
            star_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'stargazers')]")
            star_count = star_ele[0].text if star_ele else "0"
            fork_ele = i.find_elements(By.XPATH, ".//a[contains(@href,'forks')]")
            forks = fork_ele[0].text if fork_ele else "0"
            tech_ele = i.find_elements(
                By.XPATH, ".//span[@itemprop='programmingLanguage']"
            )
            technology_used = tech_ele[0].text if tech_ele else "N/A"
            # project_list[project_name] = project_url
            project_list.append(
                {
                    "Project Name": project_name,
                    "Project URL": project_url,
                    "Description": description,
                    "Stars": star_count,
                    "Forks": forks,
                    "Technology Used": technology_used,
                }
            )
        except Exception as e:
            print(f"Error extracting data: {e}")

    browser.quit()
    return pd.DataFrame(project_list)


# parallel execution of the function

# def parallel_scraping(urls):
#     """Run parallel web scraping with ThreadPoolExecutor."""
#     results = []
#     with ThreadPoolExecutor(max_workers=3) as executor:  # Adjust workers to avoid blocking
#         future_to_url = {executor.submit(scrap_git_repo, url): url for url in urls}

#         for future in as_completed(future_to_url):
#             try:
#                 result = future.result()
#                 if not result.empty:
#                     results.append(result)
#             except Exception as e:
#                 print(f"Error scraping {future_to_url[future]}: {e}")

#     return pd.concat(results, ignore_index=True) if results else pd.DataFrame()


# project_df = pd.DataFrame.from_dict(project_list, orient="index")
# project_df["project_name"] = project_df.index
# project_df.columns = ["project_url", "project_name","description","star_count","forks","techonolgy_used"]
# project_df = project_df.reset_index(drop=True)
# project_df = pd.DataFrame(project_list)

# project_df.to_csv("project_list.csv")


# if "scraped_data" not in st.session_state:
#     st.session_state.scraped_data = None

# if st.sidebar.button("Start Scraping"):
#     with st.spinner("Scraping GitHub Collection..."):
#         df = scrap_git_repo(github_url)
#         st.success("Scraping Completed!")

#         # Show Data
#         st.write("### Scraped Repositories")
#         st.dataframe(df)

#         # CSV Download Button
#         csv = df.to_csv(index=False)
#         st.download_button(
#             " Download CSV", data=csv, file_name="github_projects.csv", mime="text/csv"
#         )

#         # Filters

#         if not df.empty:
#             df["Stars"] = df["Stars"].astype(int)
#             max_stars = int(df["Stars"].max())
#             min_stars = st.sidebar.slider(
#                 "Minimum Stars:", min_value=0, max_value=max_stars, value=0
#             )

#             df_filtered = df[df["Stars"] >= min_stars]

#         # Top Repositories Visualization
#         st.write("### Top Starred Repositories")
#         fig = px.bar(
#             df_filtered.sort_values(by="Stars", ascending=False)[:10],
#             x="Stars",
#             y="Project Name",
#             orientation="h",
#             title="Top Starred Repositories",
#         )
#         st.plotly_chart(fig)

#         # if not df.empty:
#         #     df["Stars"] = df["Stars"].astype(int)
#         #     max_stars = int(df["Stars"].max())
#         #     min_stars = st.sidebar.slider(
#         #         "Minimum Stars:", min_value=0, max_value=max_stars, value=0
#         #     )

#         #     df_filtered = df[df["Stars"] >= min_stars]

#         #     # **Top Repositories Visualization**
#         #     st.write("### Top Starred Repositories")
#         #     df_filtered_sorted = df_filtered.sort_values(by="Stars", ascending=False)[
#         #         :10
#         #     ]

#         #     fig = px.bar(
#         #         df_filtered_sorted,
#         #         x="Stars",
#         #         y="Project Name",
#         #         orientation="h",
#         #         title="⭐ Top Starred GitHub Repositories",
#         #         labels={"Stars": "GitHub Stars", "Project Name": "Repository Name"},
#         #     )

#         #     fig.update_layout(
#         #         yaxis=dict(title="Repository Name", tickmode="linear"),
#         #         xaxis=dict(title="GitHub Stars", tickformat=","),
#         #         margin=dict(l=200, r=50, t=50, b=50),
#         #         height=600,
#         #     )

#         #     st.plotly_chart(fig)

#         #     df_filtered = df[df["Stars"] >= min_stars]

#         # # Top Repositories Visualization
#         # st.write("### Top Starred Repositories")
#         # df_filtered_sorted = df_filtered.sort_values(by="Stars", ascending=False)[:10]

#         # # Create the bar chart
#         # fig = px.bar(
#         #     df_filtered_sorted,
#         #     x="Stars",
#         #     y="Project Name",
#         #     orientation="h",
#         #     title="⭐ Top Starred GitHub Repositories",
#         #     labels={"Stars": "GitHub Stars", "Project Name": "Repository Name"},
#         # )

#         # fig.update_layout(
#         #     yaxis=dict(title="Repository Name", tickmode="linear"),
#         #     xaxis=dict(title="GitHub Stars", tickformat=","),
#         #     margin=dict(l=200, r=50, t=50, b=50),  # Adjust margins for better spacing
#         #     height=600,  # Increase height for better readability
#         # )

#         # st.plotly_chart(fig)

#         # fig = px.bar(
#         #     df_filtered.sort_values(by="Stars", ascending=False)[:10],
#         #     x="Stars",
#         #     y="Project Name",
#         #     orientation="h",
#         #     title="Top Starred Repositories",
#         # )
#         # st.plotly_chart(fig)

#         # Most Used Languages
#         st.write("### Most Used Technologies")
#         lang_counts = df["Technology Used"].value_counts().reset_index()
#         lang_counts.columns = ["Technology", "Count"]
#         fig2 = px.pie(
#             lang_counts,
#             names="Technology",
#             values="Count",
#             title="Most Popular Technologies",
#         )
#         st.plotly_chart(fig2)


# # st.write("Click to scrape.")

# # if st.button("Scrape"):
# #     with st.spinner("Scraping... Please wait!"):
# #         df = scrap_git_repo()
# #         st.success("Scraping completed!")
# #         st.dataframe(df)

# #         csv = df.to_csv(index=False)
# #         st.download_button(
# #             "Download CSV ", data=csv, file_name="project_lists.csv", mime="text/csv"
# #         )


if st.sidebar.button("Start Scraping"):
    with st.spinner("Scraping GitHub Collection..."):
        df = scrap_git_repo(github_url)

        if df.empty:  # If scraping failed or returned no data
            st.error("No valid data found. Please check the URL and try again.")
        else:
            st.success("Scraping Completed!")

            # Show Data
            st.write("### Scraped Repositories")
            st.dataframe(df)

            # CSV Download Button
            csv = df.to_csv(index=False)
            st.download_button(
                " Download CSV",
                data=csv,
                file_name="github_projects.csv",
                mime="text/csv",
            )

            # Filters
            df["Stars"] = (
                pd.to_numeric(df["Stars"], errors="coerce").fillna(0).astype(int)
            )
            max_stars = int(df["Stars"].max())
            min_stars = st.sidebar.slider(
                "Minimum Stars:", min_value=0, max_value=max_stars, value=0
            )

            df_filtered = df[df["Stars"] >= min_stars]

            # Check if df_filtered is empty before plotting
            if not df_filtered.empty:
                st.write("### Top Starred Repositories")
                fig = px.bar(
                    df_filtered.sort_values(by="Stars", ascending=False)[:10],
                    x="Stars",
                    y="Project Name",
                    orientation="h",
                    title="Top Starred Repositories",
                )
                st.plotly_chart(fig)

            # Most Used Technologies
            st.write("### Most Used Technologies")
            lang_counts = df["Technology Used"].value_counts().reset_index()
            lang_counts.columns = ["Technology", "Count"]
            fig2 = px.pie(
                lang_counts,
                names="Technology",
                values="Count",
                title="Most Popular Technologies",
            )
            st.plotly_chart(fig2)

